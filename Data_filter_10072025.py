# -*- coding: utf-8 -*-
"""Копия блокнота "Копия блокнота "data_filer2.ipynb""

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vURaEm6oIzT-_HsHOb_MUUaGIKqtQ8Zc
"""

!pip install swifter
!pip install ftfy


import sys
import subprocess
import importlib
import os
import pandas as pd
import re
import numpy as np
from tqdm.notebook import tqdm
import swifter
import ftfy
import time
import string

print("\nВсе библиотеки готовы к работе!")

# Определение функции - Очистка самого текста
def clean_text(text):
    """Комплексная очистка текста"""
    if not isinstance(text, str) or text.strip() == "":
        return ""

    # Восстановление кодировки и юникода
    text = ftfy.fix_text(text)

    # Сохранение критичных медицинских паттернов
    patterns_to_preserve = re.findall(
        r'\b\d+[\.,]\d+\s*[мгмлкгµмкмл%/:]+\b|'
        r'\b\d+\s*[-–—]\s*\d+\s*[ммсм%/:]+\b|'
        r'\b\d+:\d+\b|'
        r'\b[\w]+/[\w]+\b',
        text
    )

    # Удаление цифрового мусора
    text = re.sub(r'http\S+|www\S+|@\S+', '', text)  # URL/соцсети
    text = re.sub(r'\b\d{5,}\b', '', text)           # Длинные числа

    # Удаление разметки
    text = re.sub(r'<[^>]+>|{[^}]+}', '', text)      # HTML/XML/JSON


    # Нормализация спецсимволов с сохранением кириллицы и греческих букв
    text = re.sub(
        # Добавлен диапазон греческих букв: U+0370 до U+03FF
        r'[^\w\s.,!?;:()"\'°%µ±–—−•«»§№/=\u0400-\u04FF\u0370-\u03FF]',
        ' ',
        text
    )


    # Восстановление медицинских паттернов
    for pattern in patterns_to_preserve:
        # 1. Оптимизация римских цифр: "II - III" → "II-III"
        clean_pattern = re.sub(r'\s*([IVXLCDM]+)\s*-\s*([IVXLCDM]+)\s*', r'\1-\2', pattern)

        # 2. Оптимизация медицинских символов: /:–—−+°
        clean_pattern = re.sub(r'\s*([/:–—−+°])\s*', r'\1', clean_pattern)

        text = text.replace(pattern, clean_pattern)

    # Фикс для химических формул (добавлены греческие буквы)
    text = re.sub(r'([A-Za-zА-Яа-я\u0370-\u03FF])\s+([\d\(])', r'\1\2', text)
    text = re.sub(r'(\d)\s+([A-Za-zА-Яа-я\u0370-\u03FF])', r'\1\2', text)

    # Удаление лишних пробелов
    text = re.sub(r'\s{2,}', ' ', text).strip()

    return text

print("Функции очистки успешно определены!")

# Расширенный набор пунктуации для медицинских текстов
extra_punctuation = '«»‹›„“”‘’…–—•§№'
all_punctuation = set(string.punctuation + extra_punctuation)

def calculate_punctuation_ratio(text: str) -> float:
    """Вычисляет соотношение пунктуации к общему числу символов (без пробелов)"""
    if not text.strip():
        return 1.0

    total_chars = 0
    punct_count = 0

    for char in text:
        if char.isspace():
            continue
        total_chars += 1
        if char in all_punctuation:
            punct_count += 1

    return punct_count / total_chars if total_chars else 1.0

# Фильтрация строк

def clean_csv_dataset(input_path, output_path, chunksize=50000, punct_threshold=0.25):
    """
    Очистка CSV файла с параллельными текстами
    :param input_path: путь к входному CSV файлу
    :param output_path: путь для сохранения очищенных данных
    :param chunksize: размер чанка для обработки (рекомендуется 50000-100000)
    """
    # Проверка существования файла
    if not os.path.exists(input_path):
        print(f"Ошибка: файл {input_path} не найден!")
        return

    print(f"Начата обработка файла: {input_path}")
    print(f"Параметры: chunksize={chunksize}, punct_threshold={punct_threshold}")

    # Создаем заголовок для выходного файла
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("source,target\n")

    # Определяем общее количество строк
    total_rows = sum(1 for _ in open(input_path, 'r', encoding='utf-8')) - 1
    processed_rows = 0
    saved_rows = 0
    punct_removed = 0

    # Читаем файл по частям
    chunks = pd.read_csv(
        input_path,
        sep=',',
        encoding='utf-8',
        chunksize=chunksize,
        on_bad_lines='skip',
        dtype={'source': str, 'target': str}  # Гарантируем строковый тип
    )

    # Множество для отслеживания уникальных пар
    seen_pairs = set()

    # Прогресс-бар
    with tqdm(total=total_rows, desc="Обработка данных") as pbar:
        for i, chunk in enumerate(chunks):
            # Обработка структуры чанка
            if i == 0 and 'source' in chunk.columns and 'target' in chunk.columns:
                pass
            elif len(chunk.columns) >= 2:
                chunk.columns = ['source', 'target'] + list(chunk.columns[2:])
            else:
                print(f"Ошибка: в чанке {i} недостаточно колонок")
                pbar.update(len(chunk))
                continue

            processed_rows += len(chunk)

            # Очистка текста
            chunk['source'] = chunk['source'].astype(str).swifter.apply(clean_text)
            chunk['target'] = chunk['target'].astype(str).swifter.apply(clean_text)

            # Фильтрация пустых строк
            chunk = chunk[(chunk['source'] != '') & (chunk['target'] != '')]

             # Фильтрация по длине
            src_len = chunk['source'].str.split().str.len()
            tgt_len = chunk['target'].str.split().str.len()
            chunk = chunk[
                (src_len >= 1) & (src_len <= 512) &
                (tgt_len >= 1) & (tgt_len <= 512) &
                (src_len / tgt_len < 3.0) & (src_len / tgt_len > 0.33)
            ]

            # Фильтрация по соотношению пунктуации
            if not chunk.empty:
                src_punct = chunk['source'].apply(calculate_punctuation_ratio)
                tgt_punct = chunk['target'].apply(calculate_punctuation_ratio)
                punct_mask = (src_punct <= punct_threshold) & (tgt_punct <= punct_threshold)
                current_punct_removed = (~punct_mask).sum()
                punct_removed += current_punct_removed
                chunk = chunk[punct_mask]

            # Удаление строк, где source == target
            chunk = chunk[chunk['source'] != chunk['target']]

            if chunk.empty:
                pbar.update(len(chunk))
                continue


            # Удаление дубликатов
            if not chunk.empty:
                # Создаем кортежи пар для проверки уникальности
                pair_tuples = chunk.apply(lambda row: (row['source'], row['target']), axis=1)

                # Фильтрация пар, которые уже встречались
                mask = ~pair_tuples.isin(seen_pairs)
                chunk = chunk[mask]

                # Обновляем множество уникальных пар
                seen_pairs.update(pair_tuples[mask].tolist())

            # Сохраняем результат
            if not chunk.empty:
                chunk[['source', 'target']].to_csv(
                    output_path,
                    mode='a',
                    header=False,
                    index=False,
                    encoding='utf-8'
                )
                saved_rows += len(chunk)

            # Обновляем прогресс-бар
            pbar.update(len(chunk))
            pbar.set_postfix_str(f"Сохранено: {saved_rows}, Удалено пунктуации: {punct_removed}")

    print(f"\nОбработка завершена. Итоговая статистика:")
    print(f"Всего строк: {processed_rows}")
    print(f"Сохранено строк: {saved_rows} ({saved_rows/processed_rows:.1%})")
    print(f"Удалено из-за пунктуации: {punct_removed} строк")
    return saved_rows

from google.colab import files  # Ключевой импорт!
print("Пожалуйста, загрузите CSV-файл")
uploaded = files.upload()
input_file_name = list(uploaded.keys())[0]
print(f"Загружен файл: {input_file_name} ({os.path.getsize(input_file_name)/(1024*1024):.2f} MB)")

output_file_name = "cleaned_data.csv"
start_time = time.time()
saved_rows = clean_csv_dataset(
    input_path=input_file_name,
    output_path=output_file_name,
    chunksize=200000,
    punct_threshold=0.3
)
print(f"Время обработки: {time.time()-start_time:.2f} сек")

files.download(output_file_name)
print("✅ Готово! Файл скачан.")