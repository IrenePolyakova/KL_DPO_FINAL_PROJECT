import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import subprocess
import os
import json
import time
import threading
from datetime import datetime
import psutil
from sklearn.model_selection import train_test_split

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NVIDIA ML (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
try:
    import nvidia_ml_py3 as nvml
    nvml.nvmlInit()
    NVIDIA_AVAILABLE = True
except ImportError:
    NVIDIA_AVAILABLE = False
    nvml = None
except Exception as e:
    NVIDIA_AVAILABLE = False
    nvml = None

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ML –±–∏–±–ª–∏–æ—Ç–µ–∫ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
try:
    from transformers import MarianTokenizer, MarianMTModel, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq
    from datasets import Dataset
    from huggingface_hub import login, create_repo
    import torch
    import gc
    ML_LIBRARIES_AVAILABLE = True
except ImportError as e:
    ML_LIBRARIES_AVAILABLE = False
    st.warning(f"ML libraries not available: {e}. Some features may be limited.")

class TrainingMonitor:
    def __init__(self):
        self.training_stats = []
        self.is_monitoring = False
        
    def get_system_stats(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–∏—Å—Ç–µ–º—ã"""
        stats = {
            'timestamp': datetime.now(),
            'cpu_percent': psutil.cpu_percent(),
            'memory_percent': psutil.virtual_memory().percent,
            'memory_used_gb': psutil.virtual_memory().used / (1024**3),
            'memory_total_gb': psutil.virtual_memory().total / (1024**3)
        }
        
        # GPU —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        if NVIDIA_AVAILABLE:
            try:
                gpu_count = nvml.nvmlDeviceGetCount()
                for i in range(gpu_count):
                    handle = nvml.nvmlDeviceGetHandleByIndex(i)
                    memory_info = nvml.nvmlDeviceGetMemoryInfo(handle)
                    utilization = nvml.nvmlDeviceGetUtilizationRates(handle)
                    temperature = nvml.nvmlDeviceGetTemperature(handle, nvml.NVML_TEMPERATURE_GPU)
                    
                    stats[f'gpu_{i}_utilization'] = utilization.gpu
                    stats[f'gpu_{i}_memory_used'] = memory_info.used / (1024**2)  # MB
                    stats[f'gpu_{i}_memory_total'] = memory_info.total / (1024**2)  # MB
                    stats[f'gpu_{i}_memory_percent'] = (memory_info.used / memory_info.total) * 100
                    stats[f'gpu_{i}_temperature'] = temperature
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è GPU —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
        
        return stats
    
    def start_monitoring(self):
        """–ó–∞–ø—É—Å—Ç–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥"""
        self.is_monitoring = True
        self.training_stats = []
        
    def stop_monitoring(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥"""
        self.is_monitoring = False
    
    def collect_stats(self):
        """–°–æ–±—Ä–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        if self.is_monitoring:
            stats = self.get_system_stats()
            self.training_stats.append(stats)
            return stats
        return None

def parse_csv(file_path, src_lang, tgt_lang):
    """–ü–∞—Ä—Å–∏–Ω–≥ CSV —Ñ–∞–π–ª–∞"""
    df = pd.read_csv(file_path)
    if src_lang not in df.columns or tgt_lang not in df.columns:
        raise ValueError(f"CSV –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏: '{src_lang}', '{tgt_lang}'")
    return [
        {"translation": {src_lang: row[src_lang], tgt_lang: row[tgt_lang]}}
        for _, row in df.iterrows()
        if isinstance(row[src_lang], str) and isinstance(row[tgt_lang], str)
    ]

def get_available_csv_files():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö CSV —Ñ–∞–π–ª–æ–≤"""
    csv_files = []
    for file in os.listdir('.'):
        if file.endswith('.csv'):
            try:
                size = os.path.getsize(file) / (1024**2)  # MB
                csv_files.append({'name': file, 'size_mb': size})
            except:
                pass
    return csv_files

def get_training_processes():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –æ–±—É—á–µ–Ω–∏—è"""
    processes = []
    for proc in psutil.process_iter(['pid', 'name', 'cmdline', 'cpu_percent', 'memory_percent']):
        try:
            if 'train_marian' in ' '.join(proc.info['cmdline']):
                processes.append(proc.info)
        except:
            pass
    return processes

def create_training_dashboard():
    """–°–æ–∑–¥–∞—Ç—å –¥–∞—à–±–æ—Ä–¥ –æ–±—É—á–µ–Ω–∏—è"""
    st.title("üöÄ MarianMT Training Dashboard")
    
    # Sidebar –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–µ–∫
    with st.sidebar:
        st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è")
        
        # –í—ã–±–æ—Ä CSV —Ñ–∞–π–ª–∞
        csv_files = get_available_csv_files()
        if csv_files:
            csv_options = [f"{f['name']} ({f['size_mb']:.1f} MB)" for f in csv_files]
            selected_csv = st.selectbox("üìÑ –í—ã–±–µ—Ä–∏—Ç–µ CSV —Ñ–∞–π–ª", csv_options)
            csv_file = csv_files[csv_options.index(selected_csv)]['name']
        else:
            st.error("CSV —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
            return
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
        st.subheader("üéØ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏")
        model_name = st.selectbox(
            "–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å",
            ["Helsinki-NLP/opus-mt-en-ru", "Helsinki-NLP/opus-mt-ru-en"]
        )
        
        epochs = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö", 1, 10, 1)
        batch_size = st.slider("Batch size", 4, 32, 24)
        learning_rate = st.select_slider(
            "Learning rate", 
            options=[1e-6, 5e-6, 1e-5, 2e-5, 5e-5, 1e-4],
            value=5e-5,
            format_func=lambda x: f"{x:.0e}"
        )
        
        st.subheader("üèÉ –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å")
        gradient_accumulation = st.slider("Gradient accumulation steps", 1, 8, 1)
        dataloader_workers = st.slider("DataLoader workers", 1, 32, 16)
        
        st.subheader("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ")
        output_dir = st.text_input("–í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è", f"./output_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        save_steps = st.slider("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ N —à–∞–≥–æ–≤", 1000, 10000, 5000)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ü–∏–∏
        st.subheader("üîß –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ")
        use_fp16 = st.checkbox("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å FP16", True)
        gradient_checkpointing = st.checkbox("Gradient checkpointing", False)
        warmup_steps = st.slider("Warmup steps", 0, 2000, 500)
        
        # –ß–µ–∫–±–æ–∫—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
        train_on_validation = st.checkbox("–û–±—É—á–∞—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ", value=False)
    
    # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
    tab1, tab2, tab3, tab4 = st.tabs(["üéÆ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ", "üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥", "üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", "üìã –õ–æ–≥–∏"])
    
    with tab1:
        st.header("üéÆ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏–µ–º")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–∫–µ–Ω–∞ HuggingFace
            hf_token = st.text_input("HuggingFace Token", type="password")
            repo_name = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è", f"fine-tuned-marian-{datetime.now().strftime('%Y%m%d')}")
            
            if st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ", type="primary"):
                if not hf_token:
                    st.error("–í–≤–µ–¥–∏—Ç–µ HuggingFace —Ç–æ–∫–µ–Ω!")
                else:
                    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã
                    command = f"""
torchrun --nproc_per_node=2 --master_port=29503 train_marian_max_speed.py \\
  --csv {csv_file} \\
  --output_dir {output_dir} \\
  --epochs {epochs} \\
  --batch_size {batch_size} \\
  --gradient_accumulation_steps {gradient_accumulation} \\
  --learning_rate {learning_rate} \\
  --warmup_steps {warmup_steps} \\
  --dataloader_num_workers {dataloader_workers} \\
  --save_steps {save_steps} \\
  --logging_steps 100 \\
  --max_grad_norm 1.0 \\
  --weight_decay 0.01 \\
  --ddp_bucket_cap_mb 100 \\
  --save_total_limit 2
                    """.strip()
                    
                    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç–∞ --train_on_validation, –µ—Å–ª–∏ —á–µ–∫–±–æ–∫—Å –≤—ã–±—Ä–∞–Ω
                    if train_on_validation:
                        command += " \\n  --train_on_validation"
                    
                    st.code(command, language="bash")
                    st.success("–ö–æ–º–∞–Ω–¥–∞ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∞! –ó–∞–ø—É—Å—Ç–∏—Ç–µ –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ.")
        
        with col2:
            st.subheader("üìä –¢–µ–∫—É—â–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã")
            processes = get_training_processes()
            
            if processes:
                for proc in processes:
                    st.info(f"üîÑ PID: {proc['pid']}, CPU: {proc['cpu_percent']:.1f}%, RAM: {proc['memory_percent']:.1f}%")
            else:
                st.warning("–ü—Ä–æ—Ü–µ—Å—Å—ã –æ–±—É—á–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    with tab2:
        st.header("üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∞
        if 'monitor' not in st.session_state:
            st.session_state.monitor = TrainingMonitor()
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("‚ñ∂Ô∏è –ù–∞—á–∞—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥"):
                st.session_state.monitor.start_monitoring()
                st.success("–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∑–∞–ø—É—â–µ–Ω!")
        
        with col2:
            if st.button("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥"):
                st.session_state.monitor.stop_monitoring()
                st.info("–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        
        with col3:
            auto_refresh = st.checkbox("üîÑ –ê–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ (5—Å)")
        
        # –ê–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
        if auto_refresh and st.session_state.monitor.is_monitoring:
            time.sleep(5)
            st.rerun()
        
        # –¢–µ–∫—É—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        if st.session_state.monitor.is_monitoring:
            stats = st.session_state.monitor.collect_stats()
            if stats:
                # –°–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("CPU", f"{stats['cpu_percent']:.1f}%")
                
                with col2:
                    st.metric("RAM", f"{stats['memory_percent']:.1f}%", 
                             f"{stats['memory_used_gb']:.1f}/{stats['memory_total_gb']:.1f} GB")
                
                # GPU –º–µ—Ç—Ä–∏–∫–∏
                if NVIDIA_AVAILABLE:
                    gpu_keys = [k for k in stats.keys() if k.startswith('gpu_') and k.endswith('_utilization')]
                    for i, key in enumerate(gpu_keys):
                        gpu_id = key.split('_')[1]
                        with col3 if i == 0 else col4:
                            st.metric(f"GPU {gpu_id}", f"{stats[key]:.1f}%",
                                     f"{stats[f'gpu_{gpu_id}_temperature']}¬∞C")
    
    with tab3:
        st.header("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
        
        if st.session_state.monitor.training_stats:
            df_stats = pd.DataFrame(st.session_state.monitor.training_stats)
            
            # –ì—Ä–∞—Ñ–∏–∫ —É—Ç–∏–ª–∏–∑–∞—Ü–∏–∏
            fig = make_subplots(
                rows=2, cols=2,
                subplot_titles=('CPU Utilization', 'Memory Usage', 'GPU Utilization', 'GPU Memory'),
                specs=[[{"secondary_y": False}, {"secondary_y": False}],
                       [{"secondary_y": False}, {"secondary_y": False}]]
            )
            
            # CPU
            fig.add_trace(
                go.Scatter(x=df_stats['timestamp'], y=df_stats['cpu_percent'], name='CPU %'),
                row=1, col=1
            )
            
            # Memory
            fig.add_trace(
                go.Scatter(x=df_stats['timestamp'], y=df_stats['memory_percent'], name='RAM %'),
                row=1, col=2
            )
            
            # GPU (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
            if NVIDIA_AVAILABLE and 'gpu_0_utilization' in df_stats.columns:
                fig.add_trace(
                    go.Scatter(x=df_stats['timestamp'], y=df_stats['gpu_0_utilization'], name='GPU 0 %'),
                    row=2, col=1
                )
                
                fig.add_trace(
                    go.Scatter(x=df_stats['timestamp'], y=df_stats['gpu_0_memory_percent'], name='GPU 0 Mem %'),
                    row=2, col=2
                )
            
            fig.update_layout(height=600, title_text="–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
            st.plotly_chart(fig, use_container_width=True)
            
            # –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
            st.subheader("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
            st.dataframe(df_stats.describe())
        else:
            st.info("–ù–∞—á–Ω–∏—Ç–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –¥–ª—è —Å–±–æ—Ä–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏")
    
    with tab4:
        st.header("üìã –õ–æ–≥–∏ –æ–±—É—á–µ–Ω–∏—è")
        
        # –ü–æ–∏—Å–∫ –ª–æ–≥ —Ñ–∞–π–ª–æ–≤
        log_files = []
        if os.path.exists('logs'):
            for file in os.listdir('logs'):
                if file.endswith('.log'):
                    log_files.append(file)
        
        if log_files:
            selected_log = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –ª–æ–≥ —Ñ–∞–π–ª", log_files)
            
            if st.button("üîÑ –û–±–Ω–æ–≤–∏—Ç—å –ª–æ–≥–∏"):
                st.rerun()
            
            try:
                with open(f'logs/{selected_log}', 'r', encoding='utf-8') as f:
                    log_content = f.read()
                    
                # –ü–æ–∫–∞–∑–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Å—Ç—Ä–æ–∫
                lines = log_content.split('\n')
                num_lines = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å—Ç—Ä–æ–∫", 10, 200, 50)
                
                st.text_area("–õ–æ–≥–∏", '\n'.join(lines[-num_lines:]), height=400)
                
                # –ü–∞—Ä—Å–∏–Ω–≥ –º–µ—Ç—Ä–∏–∫ –∏–∑ –ª–æ–≥–æ–≤
                loss_lines = [line for line in lines if "'loss':" in line]
                if loss_lines:
                    st.subheader("üìâ –ì—Ä–∞—Ñ–∏–∫ Loss")
                    losses = []
                    steps = []
                    
                    for i, line in enumerate(loss_lines[-100:]):  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 100 –∑–∞–ø–∏—Å–µ–π
                        try:
                            # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ loss
                            loss_start = line.find("'loss': ") + 8
                            loss_end = line.find(",", loss_start)
                            if loss_end == -1:
                                loss_end = line.find("}", loss_start)
                            
                            loss_value = float(line[loss_start:loss_end])
                            losses.append(loss_value)
                            steps.append(i)
                        except:
                            continue
                    
                    if losses:
                        fig = px.line(x=steps, y=losses, title="Training Loss")
                        fig.update_xaxes(title="Step")
                        fig.update_yaxes(title="Loss")
                        st.plotly_chart(fig, use_container_width=True)
                
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –ª–æ–≥–∞: {e}")
        else:
            st.info("–õ–æ–≥ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞)
def main(args):
    if not ML_LIBRARIES_AVAILABLE:
        st.error("ML libraries (transformers, datasets, torch) are not available. Please install them to use training functionality.")
        return
    
    login(token=args.token)
    src_lang, tgt_lang = ("en", "ru") if "en-ru" in args.model_name else ("ru", "en")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ train –∏ validation
    translations = parse_csv(args.csv, src_lang, tgt_lang)
    train_data, val_data = train_test_split(translations, test_size=0.1, random_state=42)

    # –ï—Å–ª–∏ –≤—ã–±—Ä–∞–Ω —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
    if hasattr(args, 'train_on_validation') and args.train_on_validation:
        train_data = val_data.copy()
        st.warning("–í–ù–ò–ú–ê–ù–ò–ï: –û–±—É—á–µ–Ω–∏–µ –±—É–¥–µ—Ç –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ!")
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ Hugging Face Dataset
    train_dataset = Dataset.from_list(train_data)
    val_dataset = Dataset.from_list(val_data)

    tokenizer = MarianTokenizer.from_pretrained(args.model_name)

    def preprocess_function(examples):
        inputs = [ex[src_lang] for ex in examples['translation']]
        targets = [ex[tgt_lang] for ex in examples['translation']]
        model_inputs = tokenizer(inputs, max_length=args.max_length, truncation=True, padding="max_length")
        with tokenizer.as_target_tokenizer():
            labels = tokenizer(targets, max_length=args.max_length, truncation=True, padding="max_length")
        model_inputs["labels"] = labels["input_ids"]
        return model_inputs

    tokenized_train = train_dataset.map(
        preprocess_function,
        batched=True,
        batch_size=1000,
        remove_columns=train_dataset.column_names,
        load_from_cache_file=False
    )
    tokenized_val = val_dataset.map(
        preprocess_function,
        batched=True,
        batch_size=1000,
        remove_columns=val_dataset.column_names,
        load_from_cache_file=False
    )

    del train_dataset, val_dataset
    gc.collect()

    model = MarianMTModel.from_pretrained(args.model_name)
    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

    training_args = Seq2SeqTrainingArguments(
        output_dir=args.output_dir,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=1,
        learning_rate=2e-5,
        num_train_epochs=args.epochs,
        save_strategy="epoch",
        evaluation_strategy="epoch",
        load_best_model_at_end=True,
        logging_steps=100,
        fp16=args.fp16 and torch.cuda.is_available(),
        push_to_hub=True,
        hub_model_id=args.repo_name,
        hub_token=args.token,
        report_to="none",
        save_safetensors=True,
        dataloader_num_workers=args.dataloader_num_workers,
        remove_unused_columns=True,
        optim="adamw_torch",
        gradient_checkpointing=args.gradient_checkpointing,
        logging_dir="./logs",
        predict_with_generate=False
    )

    try:
        create_repo(args.repo_name, token=args.token, private=True, exist_ok=True)
    except Exception:
        pass

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_val,
        data_collator=data_collator,
        tokenizer=tokenizer
    )

    train_result = trainer.train()
    metrics = train_result.metrics
    print(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! Final loss: {metrics['train_loss']:.4f}")
    trainer.save_model("final_model")
    trainer.push_to_hub(commit_message="–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

# –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞
if __name__ == "__main__":
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–ø—É—â–µ–Ω –ª–∏ –∫–∞–∫ Streamlit –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
    try:
        # –ï—Å–ª–∏ –∑–∞–ø—É—â–µ–Ω–æ —á–µ—Ä–µ–∑ streamlit run, —Ç–æ —Å–æ–∑–¥–∞–µ–º –¥–∞—à–±–æ—Ä–¥
        if 'streamlit' in str(st.__file__):
            create_training_dashboard()
    except:
        # –ï—Å–ª–∏ –∑–∞–ø—É—â–µ–Ω–æ –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Å–∫—Ä–∏–ø—Ç, —Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º argparse
        import argparse
        
        parser = argparse.ArgumentParser()
        parser.add_argument("--csv", type=str, required=True)
        parser.add_argument("--token", type=str, required=True)
        parser.add_argument("--model_name", type=str, default="Helsinki-NLP/opus-mt-en-ru")
        parser.add_argument("--repo_name", type=str, default="fine-tuned-marian")
        parser.add_argument("--epochs", type=int, default=1)
        parser.add_argument("--batch_size", type=int, default=4)
        parser.add_argument("--max_length", type=int, default=128)
        parser.add_argument("--fp16", action="store_true")
        parser.add_argument("--gradient_checkpointing", action="store_true")
        parser.add_argument("--output_dir", type=str, default="./output")
        parser.add_argument("--dataloader_num_workers", type=int, default=2)
        parser.add_argument("--train_on_validation", action="store_true", help="–û–±—É—á–∞—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ")
        
        args = parser.parse_args()
        main(args)