import streamlit as st
import os
import zipfile
import tempfile
from transformers import MarianMTModel, MarianTokenizer
from docx import Document
from io import BytesIO
import pandas as pd

st.set_page_config(page_title="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–æ–≤ MarianMT", layout="wide")
st.title("üìÑ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–æ–≤ —Å MarianMT (—Ç–æ–ª—å–∫–æ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏)")
st.markdown("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç –∏ ZIP-—Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.")

# –í—ã–±–æ—Ä –º–æ–¥–µ–ª–µ–π (–ª–æ–∫–∞–ª—å–Ω—ã–µ —Ç–æ–ª—å–∫–æ)
num_models = st.number_input("–°–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π –≤—ã —Ö–æ—Ç–∏—Ç–µ –∑–∞–≥—Ä—É–∑–∏—Ç—å?", min_value=1, max_value=5, value=1)
model_names = [f"Model_{i+1}" for i in range(num_models)]

# –ó–∞–≥—Ä—É–∑–∫–∞ zip-—Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–µ–π
uploaded_finetuned_paths = {}
st.markdown("### üîÑ –ó–∞–≥—Ä—É–∑–∏—Ç–µ ZIP-—Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–µ–π")
for model in model_names:
    uploaded_zip = st.file_uploader(f"ZIP-—Ñ–∞–π–ª –º–æ–¥–µ–ª–∏ {model}", type="zip", key=f"upload_{model}")
    if uploaded_zip is not None:
        with tempfile.TemporaryDirectory() as tmpdir:
            zip_path = os.path.join(tmpdir, "model.zip")
            with open(zip_path, "wb") as f:
                f.write(uploaded_zip.read())

            extract_path = os.path.join("uploaded_finetuned_models", model)
            os.makedirs(extract_path, exist_ok=True)

            with zipfile.ZipFile(zip_path, "r") as zip_ref:
                zip_ref.extractall(extract_path)

            uploaded_finetuned_paths[model] = extract_path
        st.success(f"–ú–æ–¥–µ–ª—å {model} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω–∞.")

# –ó–∞–≥—Ä—É–∑–∫–∞ DOCX
uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ DOCX-–¥–æ–∫—É–º–µ–Ω—Ç", type=["docx"])

if uploaded_file and uploaded_finetuned_paths:
    doc = Document(uploaded_file)
    paragraphs = [para.text.strip() for para in doc.paragraphs if para.text.strip()]
    if not paragraphs:
        st.warning("–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞.")
    else:
        st.info(f"–ù–∞–π–¥–µ–Ω–æ –∞–±–∑–∞—Ü–µ–≤ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞: {len(paragraphs)}")

        translations = {model: [] for model in uploaded_finetuned_paths}
        translations["Original"] = paragraphs

        for model_name, model_path in uploaded_finetuned_paths.items():
            st.write(f"üîÑ –ü–µ—Ä–µ–≤–æ–¥ —Å –ø–æ–º–æ—â—å—é {model_name}...")

            try:
                tokenizer = MarianTokenizer.from_pretrained(model_path)
                model = MarianMTModel.from_pretrained(model_path)
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
                continue

            translated = []
            for p in paragraphs:
                inputs = tokenizer(p, return_tensors="pt", padding=True, truncation=True)
                outputs = model.generate(**inputs)
                translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                translated.append(translated_text)
            translations[model_name] = translated

        # –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        st.markdown("## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–æ–≤")
        df = pd.DataFrame(translations)
        st.dataframe(df, use_container_width=True)

        # –í—ã–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        st.markdown("### üì• –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
        export_format = st.selectbox("–§–æ—Ä–º–∞—Ç –≤—ã–≥—Ä—É–∑–∫–∏", ["DOCX", "CSV"])
        if export_format == "CSV":
            csv = df.to_csv(index=False).encode("utf-8")
            st.download_button("–°–∫–∞—á–∞—Ç—å CSV", data=csv, file_name="translations.csv", mime="text/csv")
        else:
            out_doc = Document()
            table = out_doc.add_table(rows=1, cols=len(df.columns))
            hdr_cells = table.rows[0].cells
            for i, col in enumerate(df.columns):
                hdr_cells[i].text = col
            for _, row in df.iterrows():
                row_cells = table.add_row().cells
                for i, val in enumerate(row):
                    row_cells[i].text = val
            docx_io = BytesIO()
            out_doc.save(docx_io)
            st.download_button("–°–∫–∞—á–∞—Ç—å DOCX", data=docx_io.getvalue(), file_name="translations.docx", mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document")
