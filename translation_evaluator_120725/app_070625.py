# pylint: disable=line-too-long,too-many-locals,too-many-statements

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Streamlit –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø–µ—Ä–≤–æ–π
import streamlit as st

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –º–æ–¥–µ–ª–µ–π –∏ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–æ–≤",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

import os
import sys
import multiprocessing
import logging
import time
from tqdm import tqdm
from docx import Document
import gc
import xml.etree.ElementTree as ET
import hashlib
from difflib import SequenceMatcher
import asyncio
import signal
import platform
import nest_asyncio
import warnings
import io
import csv
import datetime
from fpdf import FPDF
import zipfile
import re
import traceback
import pandas as pd
import tempfile
import seaborn as sns
import matplotlib.pyplot as plt
import codecs
import torch
from translation_utils.model_loader import load_model_from_zip
from translation_utils.translator import translate_texts
from translation_utils.metrics import compute_metrics_batch
from translation_utils.docx_utils import extract_content_from_docx
import docx
from langdetect import detect

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –∫–æ—Ä–Ω–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path
sys.path.insert(0, os.path.abspath(os.path.dirname(os.path.dirname(__file__))))

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –º–æ–¥—É–ª–∏
from translation_utils.tmx_utils import create_tmx_file, TmxCreator, TmxEntry

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

def setup_async_env():
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â–∏–π —Ü–∏–∫–ª —Å–æ–±—ã—Ç–∏–π
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ç—á –¥–ª—è –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Ü–∏–∫–ª–æ–≤ —Å–æ–±—ã—Ç–∏–π
        try:
            nest_asyncio.apply()
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–º–µ–Ω–∏—Ç—å nest_asyncio: {str(e)}")
        
        return loop
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è: {str(e)}")
        return None

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
setup_async_env()

# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
def calculate_file_hash(file_content):
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Ö–µ—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞"""
    if file_content is None:
        return None
    return hashlib.md5(file_content).hexdigest()

@st.cache_data(ttl=3600)
def get_file_content(uploaded_file):
    """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞"""
    if uploaded_file is None:
        return None
    try:
        return uploaded_file.getvalue()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
        return None

@st.cache_resource(ttl=3600)
def load_cached_model(model_path, extract_path):
    """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
    try:
        logger.info(f"–ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –∏–∑ {model_path}")
        model, tokenizer = load_model_from_zip(model_path, extract_path)
        logger.info("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        return model, tokenizer
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        logger.error(traceback.format_exc())
        return None, None

def process_uploaded_file(file):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
    try:
        if file.type == "text/csv":
            df = pd.read_csv(file)
            if 'text' not in df.columns:
                st.error("CSV —Ñ–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫—É 'text'")
                return None
            return df['text'].tolist()
        else:
            result = extract_content_from_docx(file)
            return result['sentences'] if result else None
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {str(e)}")
        return None

def cleanup_cuda():
    """–û—á–∏—â–∞–µ—Ç CUDA –ø–∞–º—è—Ç—å"""
    if torch.cuda.is_available():
        try:
            torch.cuda.empty_cache()
            gc.collect()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ CUDA –ø–∞–º—è—Ç–∏: {str(e)}")

def safe_division(x, y, default=0.0):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –¥–µ–ª–µ–Ω–∏–µ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å"""
    try:
        return x / y if y != 0 else default
    except:
        return default

def clean_text_for_export(text):
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç escape-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞"""
    if not isinstance(text, str):
        return str(text)
    
    try:
        text = bytes(text, 'utf-8').decode('unicode_escape')
    except:
        pass
    
    text = text.replace('\\r', '').replace('\\n', ' ')
    text = text.replace('\\t', ' ').replace('\\xa0', ' ')
    text = ' '.join(text.split())
    
    return text

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Å—Å–∏–∏
if 'initialized' not in st.session_state:
    st.session_state.initialized = True
    st.session_state.update({
        'model1': None,
        'model2': None,
        'tokenizer1': None,
        'tokenizer2': None,
        'source_texts': [],
        'reference_texts': [],
        'translations1': [],
        'translations2': [],
        'yandex_translations': [],
        'google_translations': [],
        'deepl_translations': [],
        'metrics': {},
        'corpus_metrics': {},
        'error': None,
        'session_id': hashlib.md5(str(time.time()).encode()).hexdigest()
    })

def init_streamlit():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Streamlit"""
    try:
        # –°–∫—Ä—ã–≤–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Å—Ç–∏–ª–∏ Streamlit
        hide_streamlit_style = """
            <style>
            #MainMenu {visibility: hidden;}
            footer {visibility: hidden;}
            </style>
        """
        st.markdown(hide_streamlit_style, unsafe_allow_html=True)
        
        return True
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Streamlit: {str(e)}")
        return False

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Streamlit –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
init_streamlit()

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è Streamlit
st.set_option('client.showErrorDetails', False)
st.set_option('client.toolbarMode', 'minimal')

def extract_content_from_csv(file_content) -> dict:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏–∑ CSV —Ñ–∞–π–ª–∞"""
    result = {'sentences': [], 'references': [], 'tables': []}

    try:
        # –ß–∏—Ç–∞–µ–º CSV —Ñ–∞–π–ª
        csv_data = pd.read_csv(io.BytesIO(file_content))
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
        required_columns = ['source', 'target']
        if not all(col in csv_data.columns for col in required_columns):
            raise ValueError("CSV —Ñ–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±—Ü—ã 'source' –∏ 'target'")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å—ã
        result['sentences'] = csv_data['source'].tolist()
        result['references'] = csv_data['target'].tolist()
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ CSV —Ñ–∞–π–ª–∞: {e}")
        
    return result

def export_to_csv(data: list, filename: str) -> str | None:
    """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ CSV —Ñ–∞–π–ª"""
    try:
        with io.StringIO() as buffer:
            writer = csv.writer(buffer)
            writer.writerows(data)
            return buffer.getvalue()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ –≤ CSV: {e}")
        return None

def export_to_pdf(data: list, filename: str) -> bytes | None:
    """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ PDF —Ñ–∞–π–ª"""
    try:
        pdf = FPDF()
        pdf.add_page()
        pdf.add_font('DejaVu', '', 'DejaVuSansCondensed.ttf', uni=True)
        pdf.set_font('DejaVu', '', 14)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        for row in data:
            pdf.multi_cell(0, 10, str(row))
            pdf.ln()
            
        return pdf.output(dest='S').encode('latin1')
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ –≤ PDF: {e}")
        return None

def export_to_docx(data, filename) -> bytes | None:
    """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ DOCX —Ñ–∞–π–ª"""
    try:
        doc = Document()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        for row in data:
            doc.add_paragraph(str(row))
            
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π –±—É—Ñ–µ—Ä
        buffer = io.BytesIO()
        doc.save(buffer)
        return buffer.getvalue()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ –≤ DOCX: {e}")
        return None

def export_to_tmx(source_texts, target_texts, filename) -> str | None:
    """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –≤ TMX —Ñ–∞–π–ª"""
    try:
        tmx_creator = TmxCreator()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–ø–∏—Å–∏ –≤ TMX
        for src, tgt in zip(source_texts, target_texts):
            entry = TmxEntry(src.strip(), tgt.strip())
            tmx_creator.add_entry(entry)
            
        # –°–æ–∑–¥–∞–µ–º TMX —Ñ–∞–π–ª
        return tmx_creator.create_tmx_content()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ –≤ TMX: {e}")
        return None

def create_metrics_plot(metrics_data) -> bytes | None:
    """–°–æ–∑–¥–∞–µ—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é –º–µ—Ç—Ä–∏–∫"""
    try:
        plt.figure(figsize=(10, 6))
        sns.boxplot(data=pd.DataFrame(metrics_data))
        plt.title('Distribution of Translation Metrics')
        plt.xticks(rotation=45)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π –±—É—Ñ–µ—Ä
        buffer = io.BytesIO()
        plt.savefig(buffer, format='png', bbox_inches='tight')
        plt.close()
        return buffer.getvalue()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞: {e}")
        return None

def create_comparison_plot(metrics_data) -> io.BytesIO | None:
    """–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫"""
    try:
        plt.figure(figsize=(12, 6))
        df = pd.DataFrame(metrics_data).round(4)
        df.plot(kind='bar')
        plt.title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞')
        plt.xlabel('–ú–µ—Ç—Ä–∏–∫–∞')
        plt.ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ')
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        buf = io.BytesIO()
        plt.savefig(buf, format='png')
        plt.close()
        return buf
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞: {str(e)}")
        return None

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    st.title("\U0001F4C4 –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –º–æ–¥–µ–ª–µ–π –∏ –º–∞—à–∏–Ω–Ω—ã—Ö –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–æ–≤")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏ CSV
    uploaded_docx = st.file_uploader("\U0001F4E5 –ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏—Å—Ö–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç (Word –∏–ª–∏ CSV)", type=["docx", "csv"])
    uploaded_ref = st.file_uploader("\U0001F4E5 –ó–∞–≥—Ä—É–∑–∏—Ç–µ —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)", type=["docx", "csv"])

    model1_zip = st.file_uploader("\U0001F9E0 –ú–æ–¥–µ–ª—å 1 (–¥–æ–æ–±—É—á–µ–Ω–Ω–∞—è)", type=["zip"])
    model2_zip = st.file_uploader("\U0001F9E0 –ú–æ–¥–µ–ª—å 2 (–∏—Å—Ö–æ–¥–Ω–∞—è)", type=["zip"])

    yandex_docx = st.file_uploader("\U0001F916 –ü–µ—Ä–µ–≤–æ–¥ –Ø–Ω–¥–µ–∫—Å (DOCX –∏–ª–∏ CSV, –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)", type=["docx", "csv"])
    google_docx = st.file_uploader("\U0001F916 –ü–µ—Ä–µ–≤–æ–¥ Google (DOCX –∏–ª–∏ CSV, –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)", type=["docx", "csv"])
    deepl_docx = st.file_uploader("\U0001F916 –ü–µ—Ä–µ–≤–æ–¥ DeepL (DOCX –∏–ª–∏ CSV, –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)", type=["docx", "csv"])

    model1_name = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ –ú–æ–¥–µ–ª–∏ 1", value="Model 1")
    model2_name = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ –ú–æ–¥–µ–ª–∏ 2", value="Model 2")

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    if uploaded_docx:
        source_texts = process_uploaded_file(uploaded_docx)
        if source_texts:
            st.session_state.source_texts = source_texts
            st.write(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏—Å—Ö–æ–¥–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {len(source_texts)}")

    if uploaded_ref:
        reference_texts = process_uploaded_file(uploaded_ref)
        if reference_texts:
            st.session_state.reference_texts = reference_texts
            st.write(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –ø–µ—Ä–µ–≤–æ–¥–æ–≤: {len(reference_texts)}")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
    if model1_zip and not st.session_state.model1:
        st.session_state.model1, st.session_state.tokenizer1 = load_cached_model(model1_zip, None)

    if model2_zip and not st.session_state.model2:
        st.session_state.model2, st.session_state.tokenizer2 = load_cached_model(model2_zip, None)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –æ—Ç –¥—Ä—É–≥–∏—Ö —Å–∏—Å—Ç–µ–º
    if yandex_docx:
        st.session_state.yandex_translations = process_uploaded_file(yandex_docx)
        
    if google_docx:
        st.session_state.google_translations = process_uploaded_file(google_docx)
        
    if deepl_docx:
        st.session_state.deepl_translations = process_uploaded_file(deepl_docx)

    # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –∏ –æ—Ü–µ–Ω–∫–∏
    col1, col2 = st.columns([1, 3])
    with col1:
        start_button = st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å", use_container_width=True)
    with col2:
        st.markdown("–ù–∞–∂–º–∏—Ç–µ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –∏ –æ—Ü–µ–Ω–∫–∏")

    if start_button:
        if not st.session_state.source_texts:
            st.error("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏—Å—Ö–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç!")
        else:
            try:
                # –ü–µ—Ä–µ–≤–æ–¥ —Å –ø–æ–º–æ—â—å—é –ú–æ–¥–µ–ª–∏ 1
                if st.session_state.model1 and st.session_state.tokenizer1:
                    with st.spinner(f"–ü–µ—Ä–µ–≤–æ–¥ —Å –ø–æ–º–æ—â—å—é {model1_name}..."):
                        translations1 = translate_texts(
                            st.session_state.source_texts,
                            st.session_state.model1,
                            st.session_state.tokenizer1
                        )
                        st.session_state.translations1 = translations1
                        
                        if st.session_state.reference_texts:
                            metrics1 = compute_metrics_batch(translations1, st.session_state.reference_texts)
                            st.session_state.metrics[model1_name] = {k: sum(v)/len(v) for k, v in metrics1.items()}
                
                # –ü–µ—Ä–µ–≤–æ–¥ —Å –ø–æ–º–æ—â—å—é –ú–æ–¥–µ–ª–∏ 2
                if st.session_state.model2 and st.session_state.tokenizer2:
                    with st.spinner(f"–ü–µ—Ä–µ–≤–æ–¥ —Å –ø–æ–º–æ—â—å—é {model2_name}..."):
                        translations2 = translate_texts(
                            st.session_state.source_texts,
                            st.session_state.model2,
                            st.session_state.tokenizer2
                        )
                        st.session_state.translations2 = translations2
                        
                        if st.session_state.reference_texts:
                            metrics2 = compute_metrics_batch(translations2, st.session_state.reference_texts)
                            st.session_state.metrics[model2_name] = {k: sum(v)/len(v) for k, v in metrics2.items()}
                
                # –û—Ü–µ–Ω–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –æ—Ç –¥—Ä—É–≥–∏—Ö —Å–∏—Å—Ç–µ–º
                if st.session_state.reference_texts:
                    if st.session_state.yandex_translations:
                        metrics_yandex = compute_metrics_batch(
                            st.session_state.yandex_translations,
                            st.session_state.reference_texts
                        )
                        st.session_state.metrics['Yandex'] = {k: sum(v)/len(v) for k, v in metrics_yandex.items()}
                    
                    if st.session_state.google_translations:
                        metrics_google = compute_metrics_batch(
                            st.session_state.google_translations,
                            st.session_state.reference_texts
                        )
                        st.session_state.metrics['Google'] = {k: sum(v)/len(v) for k, v in metrics_google.items()}
                    
                    if st.session_state.deepl_translations:
                        metrics_deepl = compute_metrics_batch(
                            st.session_state.deepl_translations,
                            st.session_state.reference_texts
                        )
                        st.session_state.metrics['DeepL'] = {k: sum(v)/len(v) for k, v in metrics_deepl.items()}
                
                st.success("‚úÖ –ü–µ—Ä–µ–≤–æ–¥ –∏ –æ—Ü–µ–Ω–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã!")
                
            except Exception as e:
                st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ –∏ –æ—Ü–µ–Ω–∫–µ: {str(e)}")

    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if st.session_state.metrics:
        st.subheader("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏")
        
        # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        comparison_df = pd.DataFrame(st.session_state.metrics).round(4)
        st.table(comparison_df)
        
        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        plot_buf = create_comparison_plot(st.session_state.metrics)
        if plot_buf:
            st.image(plot_buf)

    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–æ–≤
    if st.session_state.source_texts:
        st.subheader("üìù –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–æ–≤")
        
        # –°–æ–∑–¥–∞–µ–º DataFrame —Å –ø–µ—Ä–µ–≤–æ–¥–∞–º–∏
        translations_data = {}
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        for i, text in enumerate(st.session_state.source_texts):
            try:
                lang = detect(text)
                if lang == 'en':
                    translations_data['–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç (en)'] = st.session_state.source_texts
                elif lang == 'ru':
                    translations_data['–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç (ru)'] = st.session_state.source_texts
                else:
                    translations_data['–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç'] = st.session_state.source_texts
                break
            except:
                translations_data['–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç'] = st.session_state.source_texts
                break
        
        if st.session_state.reference_texts:
            translations_data['–≠—Ç–∞–ª–æ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ (–∏–∑ —Ñ–∞–π–ª–∞)'] = st.session_state.reference_texts
        
        if st.session_state.translations1:
            translations_data[f"{model1_name} (–∏–∑ —Ñ–∞–π–ª–∞)"] = st.session_state.translations1
        if st.session_state.translations2:
            translations_data[f"{model2_name} (–∏–∑ —Ñ–∞–π–ª–∞)"] = st.session_state.translations2
        if st.session_state.yandex_translations:
            translations_data['Yandex (API)'] = st.session_state.yandex_translations
        if st.session_state.google_translations:
            translations_data['Google (API)'] = st.session_state.google_translations
        if st.session_state.deepl_translations:
            translations_data['DeepL (API)'] = st.session_state.deepl_translations
        
        translations_df = pd.DataFrame(translations_data)
        st.dataframe(translations_df)

if __name__ == "__main__":
    main()