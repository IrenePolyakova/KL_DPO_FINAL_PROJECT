import os
import sys
import streamlit as st
import warnings

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –¥–ª—è torch path
warnings.filterwarnings('ignore', message='.*torch.classes.*')

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–æ –ª—é–±—ã—Ö –¥—Ä—É–≥–∏—Ö –≤—ã–∑–æ–≤–æ–≤ Streamlit
st.set_page_config(
    page_title="–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# –ò–º–ø–æ—Ä—Ç—ã –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥—É–ª–µ–π
from translation_utils.model_loader import load_model_from_zip
from translation_utils.translator import translate_texts
from translation_utils.metrics import compute_corpus_metrics
from translation_utils.docx_utils import extract_content_from_docx, align_sentences
import pandas as pd
import tempfile
import seaborn as sns
import matplotlib.pyplot as plt
import io
import csv
import codecs
import datetime
from fpdf import FPDF
import zipfile
import re
import traceback
import torch
import gc
import logging
import time
from tqdm import tqdm
from docx import Document
import sacrebleu
from langdetect import detect

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
def init_state():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    if 'initialized' not in st.session_state:
        st.session_state.initialized = True
        st.session_state.processed_systems = []
        st.session_state.translations = {}
        st.session_state.metrics = {}
        st.session_state.src_sentences = []
        st.session_state.ref_sentences = []
        st.session_state.df = None
        st.session_state.model_speeds = {}
        st.session_state.error_occurred = False
        st.session_state.error_message = ""
        st.session_state.progress = 0
        st.session_state.progress_text = ""

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Streamlit
def init_streamlit():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Streamlit"""
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
        init_state()
        
        # –°–∫—Ä—ã–≤–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Å—Ç–∏–ª–∏ Streamlit
        hide_streamlit_style = """
            <style>
            #MainMenu {visibility: hidden;}
            footer {visibility: hidden;}
            </style>
        """
        st.markdown(hide_streamlit_style, unsafe_allow_html=True)
        
        return True
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Streamlit: {str(e)}")
        return False

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Streamlit –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
init_streamlit()

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è Streamlit
st.set_option('client.showErrorDetails', False)
st.set_option('client.toolbarMode', 'minimal')

# –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
@st.cache_resource(ttl=3600)
def load_cached_model(model_zip, model_name):
    """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
    try:
        logger.info(f"–ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}")
        with tempfile.TemporaryDirectory() as tmpdir:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            temp_zip_path = os.path.join(tmpdir, f"{model_name}.zip")
            with open(temp_zip_path, "wb") as f:
                f.write(model_zip.getvalue())
            
            model, tokenizer = load_model_from_zip(temp_zip_path, os.path.join(tmpdir, model_name))
            logger.info(f"–ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return model, tokenizer
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
        logger.error(traceback.format_exc())
        return None, None

@st.cache_data(ttl=3600)
def process_file_content(file, file_type):
    """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞"""
    if file is None:
        return {'sentences': [], 'references': [], 'tables': []}
    
    try:
        if file_type == 'csv':
            return extract_content_from_csv(file)
        else:
            return extract_content_from_docx(file)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {e}")
        return {'sentences': [], 'references': [], 'tables': []}

def clear_gpu_memory():
    """–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ GPU"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()

# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–µ—Ä–µ–≤–æ–¥–æ–≤
def process_model_translation(system_name, model_zip, texts):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –¥–ª—è –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –∑–∞–≥—Ä—É–∑–∫–æ–π –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –ø–∞–º—è—Ç—å—é"""
    try:
        logger.info(f"–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –¥–ª—è —Å–∏—Å—Ç–µ–º—ã {system_name}")
        start_time = time.time()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        if not model_zip or not texts:
            logger.error(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è {system_name}: model_zip={bool(model_zip)}, texts={bool(texts)}")
            return None
            
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        model, tokenizer = load_cached_model(model_zip, system_name)
        if model is None or tokenizer is None:
            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å {system_name}")
            return None
            
        # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å GPU –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–≤–æ–¥–æ–º
        clear_gpu_memory()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –∂–µ–ª–µ–∑–∞
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞:
        # - –î–ª—è GPU (GTX 1050 Ti 4GB): –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–Ω—å—à–∏–π –±–∞—Ç—á –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏
        # - –î–ª—è CPU (56 –ø–æ—Ç–æ–∫–æ–≤): –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª—å—à–∏–π –±–∞—Ç—á –¥–ª—è –ª—É—á—à–µ–π —É—Ç–∏–ª–∏–∑–∞—Ü–∏–∏
        batch_size = 4 if device == "cuda" else 32
        
        # –î–ª—è CPU –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª—å—à–µ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ (—É –Ω–∞—Å 56 –ø–æ—Ç–æ–∫–æ–≤)
        n_jobs = 1 if device == "cuda" else min(56, len(texts) // 1000 + 1)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–µ—Ä–µ–≤–æ–¥ —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
        logger.info(f"–ù–∞—á–∞–ª–æ –ø–µ—Ä–µ–≤–æ–¥–∞ {len(texts)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é {system_name}")
        logger.info(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: device={device}, batch_size={batch_size}, n_jobs={n_jobs}")
        
        translations, speed = translate_texts(
            texts=texts,
            model=model,
            tokenizer=tokenizer,
            batch_size=batch_size,
            max_length=512,
            num_beams=4,
            device=device,
            show_progress=True,
            n_jobs=n_jobs
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–µ—Ä–µ–≤–æ–¥–∞
        if not translations:
            logger.error(f"–ù–µ –ø–æ–ª—É—á–µ–Ω–æ –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –æ—Ç –º–æ–¥–µ–ª–∏ {system_name}")
            return None
            
        if len(translations) != len(texts):
            logger.error(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä–µ–≤–æ–¥–æ–≤ ({len(translations)}) –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ ({len(texts)})")
            return None
        
        # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –ø–µ—Ä–µ–≤–æ–¥–∞
        logger.info("–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –ø–µ—Ä–µ–≤–æ–¥–∞")
        del model
        del tokenizer
        torch.cuda.empty_cache()
        gc.collect()
        
        logger.info(f"–ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {time.time() - start_time:.2f} —Å–µ–∫, —Å–∫–æ—Ä–æ—Å—Ç—å: {speed:.2f} –ø—Ä–µ–¥–ª./—Å–µ–∫")
        return translations
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –¥–ª—è {system_name}: {str(e)}")
        logger.error(traceback.format_exc())
        return None

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ PDF

def generate_pdf_report(df, metrics_df, fig):
    pdf = FPDF()
    pdf.add_page()
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —à—Ä–∏—Ñ—Ç 'helvetica'
    pdf.set_font("helvetica", size=12)
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    pdf.set_font("helvetica", 'B', 16)
    pdf.cell(200, 10, txt="–û—Ç—á–µ—Ç –æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å–∏—Å—Ç–µ–º –ø–µ—Ä–µ–≤–æ–¥–∞", ln=1, align='C')
    pdf.set_font("helvetica", size=12)
    pdf.cell(200, 10, txt=f"–î–∞—Ç–∞: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", ln=1)
    pdf.ln(10)
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–æ—Ä–ø—É—Å—É
    if metrics_df is not None:
        pdf.set_font("helvetica", 'B', 14)
        pdf.cell(200, 10, txt="–ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–æ—Ä–ø—É—Å—É", ln=1)
        pdf.set_font("helvetica", size=10)
        
        # –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Ç–∞–±–ª–∏—Ü—ã
        col_width = 40
        pdf.cell(col_width, 10, "–°–∏—Å—Ç–µ–º–∞", border=1)
        pdf.cell(col_width, 10, "BLEU", border=1)
        pdf.cell(col_width, 10, "chrF", border=1)
        pdf.cell(col_width, 10, "TER", border=1)
        pdf.ln()
        
        # –î–∞–Ω–Ω—ã–µ
        for _, row in metrics_df.iterrows():
            pdf.cell(col_width, 10, str(row['–°–∏—Å—Ç–µ–º–∞']), border=1)
            pdf.cell(col_width, 10, f"{row['BLEU']:.2f}", border=1)
            pdf.cell(col_width, 10, f"{row['chrF']:.2f}", border=1)
            pdf.cell(col_width, 10, f"{row['TER']:.2f}", border=1)
            pdf.ln()
        pdf.ln(10)
    
    # –ì—Ä–∞—Ñ–∏–∫
    if fig is not None:
        pdf.set_font("helvetica", 'B', 14)
        pdf.cell(200, 10, txt="–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫", ln=1)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫ –≤ –±—É—Ñ–µ—Ä –ø–∞–º—è—Ç–∏
        img_buffer = io.BytesIO()
        fig.savefig(img_buffer, format='png', bbox_inches='tight')
        img_buffer.seek(0)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        with tempfile.NamedTemporaryFile(suffix='.png', delete=True) as tmpfile:
            tmpfile.write(img_buffer.getvalue())
            tmpfile.flush()
            pdf.image(tmpfile.name, x=10, y=pdf.get_y(), w=180)
        
        pdf.ln(90)
    
    # –ü–æ—è—Å–Ω–µ–Ω–∏—è –∫ –º–µ—Ç—Ä–∏–∫–∞–º
    pdf.set_font("helvetica", 'B', 14)
    pdf.cell(200, 10, txt="–ü–æ—è—Å–Ω–µ–Ω–∏—è –∫ –º–µ—Ç—Ä–∏–∫–∞–º", ln=1)
    pdf.set_font("helvetica", size=10)
    explanations = [
        "BLEU (Bilingual Evaluation Understudy) - –º–µ—Ç—Ä–∏–∫–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ n-–≥—Ä–∞–º–º. "
        "–£—á–∏—Ç—ã–≤–∞–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Å–ª–æ–≤ –º–µ–∂–¥—É –ø–µ—Ä–µ–≤–æ–¥–æ–º –∏ —ç—Ç–∞–ª–æ–Ω–æ–º. "
        "–ó–Ω–∞—á–µ–Ω–∏–µ –æ—Ç 0 –¥–æ 100, —á–µ–º –≤—ã—à–µ, —Ç–µ–º –ª—É—á—à–µ.",
        
        "chrF (Character n-gram F-score) - –º–µ—Ç—Ä–∏–∫–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ F-–º–µ—Ä–µ –¥–ª—è —Å–∏–º–≤–æ–ª—å–Ω—ã—Ö n-–≥—Ä–∞–º–º (–æ–±—ã—á–Ω–æ n=6). "
        "–£—á–∏—Ç—ã–≤–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –ø–æ–ª–Ω–æ—Ç—É –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–∏–º–≤–æ–ª–æ–≤. "
        "–ó–Ω–∞—á–µ–Ω–∏–µ –æ—Ç 0 –¥–æ 1 (–∏–ª–∏ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö), —á–µ–º –≤—ã—à–µ, —Ç–µ–º –ª—É—á—à–µ.",
        
        "TER (Translation Edit Rate) - –º–µ—Ç—Ä–∏–∫–∞, –∏–∑–º–µ—Ä—è—é—â–∞—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–π "
        "(–≤—Å—Ç–∞–≤–∫–∞, —É–¥–∞–ª–µ–Ω–∏–µ, –∑–∞–º–µ–Ω–∞, –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–ª–æ–≤), –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞ –≤ —ç—Ç–∞–ª–æ–Ω, "
        "–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞ –¥–ª–∏–Ω—É —ç—Ç–∞–ª–æ–Ω–∞. –ß–µ–º –Ω–∏–∂–µ –∑–Ω–∞—á–µ–Ω–∏–µ, —Ç–µ–º –ª—É—á—à–µ."
    ]
    
    for exp in explanations:
        pdf.multi_cell(0, 5, txt=exp)
        pdf.ln(3)
    
    return pdf.output(dest='S')

def create_zip_archive(src_sentences, ref_sentences, systems, tables_systems, df, metrics_df, fig, file_prefix):
    """–°–æ–∑–¥–∞–µ—Ç ZIP-–∞—Ä—Ö–∏–≤ —Å–æ –≤—Å–µ–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏"""
    zip_buffer = io.BytesIO()
    
    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
        # –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        zip_file.writestr(f"source_{file_prefix}.txt", "\n".join(src_sentences))
        
        if ref_sentences:
            zip_file.writestr(f"reference_{file_prefix}.txt", "\n".join(ref_sentences))
        
        # –ü–µ—Ä–µ–≤–æ–¥—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        for system in systems:
            if system in st.session_state.translations:
                zip_file.writestr(f"translation_{system}_{file_prefix}.txt", 
                                 "\n".join(st.session_state.translations[system]))
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
        csv_data = df.to_csv(index=False).encode('utf-8-sig')
        zip_file.writestr(f"comparison_{file_prefix}.csv", csv_data)
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        if metrics_df is not None:
            metrics_csv = metrics_df.to_csv(index=False).encode('utf-8-sig')
            zip_file.writestr(f"metrics_{file_prefix}.csv", metrics_csv)
            
            # –ì—Ä–∞—Ñ–∏–∫
            if fig:
                img_io = io.BytesIO()
                fig.savefig(img_io, format='png', bbox_inches='tight')
                img_bytes = img_io.getvalue()
                zip_file.writestr(f"metrics_plot_{file_prefix}.png", img_bytes)
            
            # PDF –æ—Ç—á–µ—Ç
            try:
                pdf_bytes = generate_pdf_report(df, metrics_df, fig)
                zip_file.writestr(f"report_{file_prefix}.pdf", pdf_bytes)
            except Exception:
                pass
    
    zip_buffer.seek(0)
    return zip_buffer

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ CSV
def extract_content_from_csv(file):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ –ø–µ—Ä–µ–≤–æ–¥—ã –∏–∑ CSV —Ñ–∞–π–ª–∞"""
    if file is None:
        return {'sentences': [], 'references': [], 'source_translations': [], 'tables': [], 'dataframe': None}
    
    try:
        file.seek(0)
        content = file.read()
        decoded = content.decode('utf-8-sig') if isinstance(content, bytes) else content
        
        df = pd.read_csv(io.StringIO(decoded))
        file_name = os.path.basename(file.name) if hasattr(file, 'name') else 'unknown'
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏
        columns_lower = {col.lower(): col for col in df.columns}
        translations = {}
        
        # –ò—â–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å –∞–Ω–≥–ª–∏–π—Å–∫–∏–º –∏ —Ä—É—Å—Å–∫–∏–º —Ç–µ–∫—Å—Ç–æ–º
        for col in df.columns:
            col_lower = col.lower()
            # –ê–Ω–≥–ª–∏–π—Å–∫–∏–π —Ç–µ–∫—Å—Ç
            if any(x in col_lower for x in ['source', 'src', 'en', '–∏—Å—Ö–æ–¥–Ω—ã–π', 'english']):
                translations['source'] = df[col].fillna('').tolist()
            # –†—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç/–ø–µ—Ä–µ–≤–æ–¥—ã
            elif any(x in col_lower for x in ['target', 'tgt', 'ru', '—ç—Ç–∞–ª–æ–Ω', '–ø–µ—Ä–µ–≤–æ–¥', 'russian']):
                translations[f'ru_from_{file_name}'] = df[col].fillna('').tolist()

        if not translations:
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ –¥–≤–µ –∫–æ–ª–æ–Ω–∫–∏
            if len(df.columns) >= 2:
                translations['source'] = df[df.columns[0]].fillna('').tolist()
                translations[f'ru_from_{file_name}'] = df[df.columns[1]].fillna('').tolist()
            else:
                translations['source'] = df[df.columns[0]].fillna('').tolist()

        # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        new_df = pd.DataFrame()
        new_df['ID'] = range(1, len(df) + 1)
        
        if 'source' in translations:
            new_df[f"Source ({file_name})"] = translations['source']
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ä—É—Å—Å–∫–∏–µ –ø–µ—Ä–µ–≤–æ–¥—ã
        for key, value in translations.items():
            if key.startswith('ru_from_'):
                new_df[f"Translation from {file_name}"] = value

        return {
            'sentences': translations.get('source', []),
            'references': translations.get(f'ru_from_{file_name}', []),
            'source_translations': translations.get(f'ru_from_{file_name}', []),
            'tables': [],
            'dataframe': new_df,
            'translations': translations
        }

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ CSV: {str(e)}")
        return {'sentences': [], 'references': [], 'source_translations': [], 'tables': [], 'dataframe': None}

def export_tmx(src_sentences, tgt_sentences, system_name):
    """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –ø–∞—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ TMX-—Ñ–æ—Ä–º–∞—Ç"""
    from translation_utils.tmx_utils import create_tmx_file
    import io
    tmx_io = io.BytesIO()
    create_tmx_file(
        src_sentences=src_sentences,
        tgt_sentences=tgt_sentences,
        fileobj=tmx_io,
        sys_name=system_name
    )
    tmx_io.seek(0)
    return tmx_io.read()

def display_results(
    df, systems, source_file=None, ref_file=None, model1_file=None, model2_file=None,
    yandex_file=None, google_file=None, deepl_file=None, extra_files=None, extra_names=None,
    model1_name=None, model2_name=None
):
    try:
        results_df = pd.DataFrame()
        
        if isinstance(df, dict):
            # –î–æ–±–∞–≤–ª—è–µ–º ID
            num_rows = len(df.get('sentences', []))
            results_df['ID'] = range(1, num_rows + 1)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            source_name = f"Source ({os.path.basename(source_file.name)})" if source_file else "Source"
            results_df[source_name] = df.get('sentences', [])
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–≤–æ–¥ –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞, –µ—Å–ª–∏ –µ—Å—Ç—å
            if df.get('source_translations'):
                source_trans_name = f"Translation from {os.path.basename(source_file.name)}"
                results_df[source_trans_name] = df['source_translations']
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–≤–æ–¥ –∏–∑ —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            if ref_file and df.get('references'):
                ref_name = f"Translation from {os.path.basename(ref_file.name)}"
                results_df[ref_name] = df['references']
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–≤–æ–¥—ã –æ—Ç —Å–∏—Å—Ç–µ–º –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞
            for system, file in [
                ('yandex', yandex_file),
                ('google', google_file),
                ('deepl', deepl_file)
            ]:
                if file:
                    file_name = os.path.basename(file.name)
                    try:
                        content = extract_content_from_csv(file) if file.name.endswith('.csv') else extract_content_from_docx(file)
                        if content.get('references'):
                            results_df[f"Translation from {file_name}"] = content['references']
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {file_name}: {str(e)}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–≤–æ–¥—ã –æ—Ç –º–æ–¥–µ–ª–µ–π
            if 'translations' in df:
                for system in systems:
                    if system in df['translations']:
                        file_name = None
                        if system == model1_name and model1_file:
                            file_name = os.path.basename(model1_file.name)
                        elif system == model2_name and model2_file:
                            file_name = os.path.basename(model2_file.name)
                        
                        system_name = f"Translation {system} ({file_name})" if file_name else f"Translation ({system})"
                        results_df[system_name] = df['translations'][system]
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–µ—Ä–µ–≤–æ–¥—ã
            if extra_files and extra_names:
                for file, name in zip(extra_files, extra_names):
                    if file:
                        try:
                            content = extract_content_from_csv(file) if file.name.endswith('.csv') else extract_content_from_docx(file)
                            if content.get('references'):
                                results_df[f"Translation from {os.path.basename(file.name)}"] = content['references']
                        except Exception as e:
                            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ {file.name}: {str(e)}")
        
        elif isinstance(df, pd.DataFrame):
            results_df = df.copy()
        else:
            raise ValueError("–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")

        if results_df.empty:
            st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è")
            return None

        st.dataframe(results_df, use_container_width=True)
        return results_df

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {str(e)}")
        st.error(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {str(e)}")
        return None

# –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
def main():
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Streamlit
    if not init_streamlit():
        st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.")
        return
    
    st.title("üìÑ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –º–æ–¥–µ–ª–µ–π –∏ –º–∞—à–∏–Ω–Ω—ã—Ö –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–æ–≤")
    current_env = os.environ.get("CONDA_DEFAULT_ENV", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
    python_version = f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
    st.markdown(f"""
    **Conda-–æ–∫—Ä—É–∂–µ–Ω–∏–µ:** `{current_env}`  
    **–í–µ—Ä—Å–∏—è Python:** `{python_version}`
    """)
    st.divider()
    
    col1, col2 = st.columns(2)
    with col1:
        uploaded_source = st.file_uploader("üìò –ò—Å—Ö–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)", type=["docx", "csv"])
        model1_zip = st.file_uploader("ü§ñ –î–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å (zip)", type=["zip"])
        model2_zip = st.file_uploader("üßë‚Äçüíª –ò—Å—Ö–æ–¥–Ω–∞—è –º–æ–¥–µ–ª—å (zip, –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)", type=["zip"])
    with col2:
        uploaded_ref = st.file_uploader("üìó –≠—Ç–∞–ª–æ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)", type=["docx", "csv"])
        yandex_file = st.file_uploader("üåê –ü–µ—Ä–µ–≤–æ–¥ –Ø–Ω–¥–µ–∫—Å (docx/csv, –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)", type=["docx", "csv"])
        google_file = st.file_uploader("üåê –ü–µ—Ä–µ–≤–æ–¥ Google (docx/csv, –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)", type=["docx", "csv"])
        deepl_file = st.file_uploader("üåê –ü–µ—Ä–µ–≤–æ–¥ DeepL (docx/csv, –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)", type=["docx", "csv"])
        extra_count = st.number_input("–°–∫–æ–ª—å–∫–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –∑–∞–≥—Ä—É–∑–∏—Ç—å?", min_value=0, max_value=5, value=0, step=1)
        extra_files = []
        extra_names = []
        for i in range(extra_count):
            extra_files.append(st.file_uploader(f"–î–æ–ø. –ø–µ—Ä–µ–≤–æ–¥ {i+1} (docx/csv)", type=["docx", "csv"], key=f"extra_file_{i}"))
            extra_names.append(st.text_input(f"–ù–∞–∑–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –¥–ª—è –¥–æ–ø. –ø–µ—Ä–µ–≤–æ–¥–∞ {i+1}", key=f"extra_name_{i}"))

    # –ù–∞–∑–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π
    col1, col2 = st.columns(2)
    with col1:
        model1_name = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ –ú–æ–¥–µ–ª–∏ 1", value="–î–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å")
    with col2:
        model2_name = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ –ú–æ–¥–µ–ª–∏ 2", value="–ò—Å—Ö–æ–¥–Ω–∞—è –º–æ–¥–µ–ª—å")

    if st.button("üöÄ –°—Ä–∞–≤–Ω–∏—Ç—å –ø–µ—Ä–µ–≤–æ–¥—ã"):
        if not uploaded_source or not model1_zip:
            st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∏—Å—Ö–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –∏ –¥–æ–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å.")
            return
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        def update_progress(step, total, msg):
            progress = int((step/total)*100)
            progress_bar.progress(progress)
            status_text.text(f"{msg} ({step}/{total})")
            st.session_state.progress = progress
            st.session_state.progress_text = msg
        
        try:
            # –°–±—Ä–æ—Å —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ—à–∏–±–æ–∫
            st.session_state.error_occurred = False
            st.session_state.error_message = ""
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            update_progress(1, 10, "–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
            if uploaded_source.name.endswith('.csv'):
                src_content = extract_content_from_csv(uploaded_source)
                src_sentences = src_content['sentences']  # –ë–µ—Ä–µ–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —Ç–µ–∫—Å—Ç
                if not uploaded_ref:  # –ï—Å–ª–∏ —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω –æ—Ç–¥–µ–ª—å–Ω–æ, –±–µ—Ä–µ–º –µ–≥–æ –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
                    ref_sentences = src_content['references']  # –ë–µ—Ä–µ–º —Ä—É—Å—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥
            else:
                src_content = extract_content_from_docx(uploaded_source)
                src_sentences = src_content['sentences']
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞, –µ—Å–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω –æ—Ç–¥–µ–ª—å–Ω–æ
            if uploaded_ref:
                update_progress(2, 10, "–û–±—Ä–∞–±–æ—Ç–∫–∞ —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞")
                if uploaded_ref.name.endswith('.csv'):
                    ref_content = extract_content_from_csv(uploaded_ref)
                    ref_sentences = ref_content['references']  # –ë–µ—Ä–µ–º —Ä—É—Å—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥
                else:
                    ref_content = extract_content_from_docx(uploaded_ref)
                    ref_sentences = ref_content['sentences']
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            st.session_state.src_sentences = src_sentences
            st.session_state.ref_sentences = ref_sentences
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –º–æ–¥–µ–ª–µ–π
            update_progress(3, 10, "–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π")
            with tempfile.TemporaryDirectory() as tmpdir:
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–µ—Ä–≤–æ–π –º–æ–¥–µ–ª–∏
                model1_preds = process_model_translation(model1_name, model1_zip, src_sentences)
                if model1_preds:
                    st.session_state.translations[model1_name] = model1_preds
                    st.session_state.processed_systems.append(model1_name)
                    update_progress(5, 10, f"–ü–µ—Ä–µ–≤–æ–¥ {model1_name} –∑–∞–≤–µ—Ä—à–µ–Ω")
                else:
                    st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –º–æ–¥–µ–ª–∏ {model1_name}")
                    st.session_state.error_occurred = True
                    st.session_state.error_message = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –º–æ–¥–µ–ª–∏ {model1_name}"
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ç–æ—Ä–æ–π –º–æ–¥–µ–ª–∏
                if model2_zip:
                    model2_preds = process_model_translation(model2_name, model2_zip, src_sentences)
                    if model2_preds:
                        st.session_state.translations[model2_name] = model2_preds
                        st.session_state.processed_systems.append(model2_name)
                        update_progress(6, 10, f"–ü–µ—Ä–µ–≤–æ–¥ {model2_name} –∑–∞–≤–µ—Ä—à–µ–Ω")
                    else:
                        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –º–æ–¥–µ–ª–∏ {model2_name}")
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–æ–≤ Yandex, Google, DeepL
                update_progress(7, 10, "–û–±—Ä–∞–±–æ—Ç–∫–∞ –º–∞—à–∏–Ω–Ω—ã—Ö –ø–µ—Ä–µ–≤–æ–¥–æ–≤")
                if yandex_file:
                    if yandex_file.name.endswith('.csv'):
                        yandex_content = extract_content_from_csv(yandex_file)
                        yandex_sentences = yandex_content['references']  # –ë–µ—Ä–µ–º —Ä—É—Å—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥ –∏–∑ –∫–æ–ª–æ–Ω–∫–∏ 'ru'
                    else:
                        yandex_content = extract_content_from_docx(yandex_file)
                        yandex_sentences = yandex_content['sentences']
                    st.session_state.translations['yandex'] = yandex_sentences
                    st.session_state.processed_systems.append('yandex')
                
                if google_file:
                    if google_file.name.endswith('.csv'):
                        google_content = extract_content_from_csv(google_file)
                        google_sentences = google_content['references']  # –ë–µ—Ä–µ–º —Ä—É—Å—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥ –∏–∑ –∫–æ–ª–æ–Ω–∫–∏ 'ru'
                    else:
                        google_content = extract_content_from_docx(google_file)
                        google_sentences = google_content['sentences']
                    st.session_state.translations['google'] = google_sentences
                    st.session_state.processed_systems.append('google')
                
                if deepl_file:
                    if deepl_file.name.endswith('.csv'):
                        deepl_content = extract_content_from_csv(deepl_file)
                        deepl_sentences = deepl_content['references']  # –ë–µ—Ä–µ–º —Ä—É—Å—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥ –∏–∑ –∫–æ–ª–æ–Ω–∫–∏ 'ru'
                    else:
                        deepl_content = extract_content_from_docx(deepl_file)
                        deepl_sentences = deepl_content['sentences']
                    st.session_state.translations['deepl'] = deepl_sentences
                    st.session_state.processed_systems.append('deepl')
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–≤–æ–¥–æ–≤
                for i, (extra_file, extra_name) in enumerate(zip(extra_files, extra_names)):
                    if extra_file:
                        update_progress(7+i, 10, f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {extra_name}")
                        if extra_file.name.endswith('.csv'):
                            extra_content = extract_content_from_csv(extra_file)
                            extra_sentences = extra_content['references']  # –ë–µ—Ä–µ–º —Ä—É—Å—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥ –∏–∑ –∫–æ–ª–æ–Ω–∫–∏ 'ru'
                        else:
                            extra_content = extract_content_from_docx(extra_file)
                            extra_sentences = extra_content['sentences']
                        st.session_state.translations[extra_name] = extra_sentences
                        st.session_state.processed_systems.append(extra_name)
            
            update_progress(9, 10, "–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            # –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
            data = []
            for i, src in enumerate(st.session_state.src_sentences):
                row = {'ID': i+1, '–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç': src}
                if st.session_state.ref_sentences and i < len(st.session_state.ref_sentences):
                    row['–≠—Ç–∞–ª–æ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥'] = st.session_state.ref_sentences[i]
                
                for system in st.session_state.processed_systems:
                    if system in st.session_state.translations and i < len(st.session_state.translations[system]):
                        row[f'–ü–µ—Ä–µ–≤–æ–¥ {system}'] = st.session_state.translations[system][i]
                
                data.append(row)
            
            df = pd.DataFrame(data)
            st.session_state.df = df
            
            # ---- –ö–û–†–ü–£–°–ù–´–ï –ú–ï–¢–†–ò–ö–ò ----
            metrics_df = None
            fig = None
            if st.session_state.ref_sentences:
                metrics_data = []
                for system in st.session_state.processed_systems:
                    try:
                        preds = st.session_state.translations[system]
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–ª–∏–Ω
                        if len(preds) != len(st.session_state.ref_sentences):
                            logger.warning(
                                f"–†–∞–∑–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –¥–ª—è —Å–∏—Å—Ç–µ–º—ã {system}: "
                                f"–ø–µ—Ä–µ–≤–æ–¥—ã={len(preds)}, —ç—Ç–∞–ª–æ–Ω={len(st.session_state.ref_sentences)}"
                            )
                            continue
                        
                        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                        valid_pairs = [
                            (pred, ref) for pred, ref in zip(preds, st.session_state.ref_sentences)
                            if pred and pred.strip() and ref and ref.strip()
                        ]
                        
                        if not valid_pairs:
                            logger.warning(f"–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –ø–∞—Ä –ø–µ—Ä–µ–≤–æ–¥-—ç—Ç–∞–ª–æ–Ω –¥–ª—è —Å–∏—Å—Ç–µ–º—ã {system}")
                            continue
                            
                        filtered_preds, filtered_refs = zip(*valid_pairs)
                        
                        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
                        bleu = sacrebleu.corpus_bleu(filtered_preds, [filtered_refs]).score
                        chrf = sacrebleu.corpus_chrf(filtered_preds, [filtered_refs]).score
                        ter = sacrebleu.corpus_ter(filtered_preds, [filtered_refs]).score
                        
                        metrics_data.append({
                            '–°–∏—Å—Ç–µ–º–∞': system,
                            'BLEU': round(bleu,2),
                            'chrF': round(chrf,2),
                            'TER': round(ter,2)
                        })
                        
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –º–µ—Ç—Ä–∏–∫ –¥–ª—è —Å–∏—Å—Ç–µ–º—ã {system}: {str(e)}")
                        continue
                
                if metrics_data:
                    metrics_df = pd.DataFrame(metrics_data)
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
                    if all(col in metrics_df.columns for col in ['–°–∏—Å—Ç–µ–º–∞', 'BLEU', 'chrF', 'TER']):
                        fig, ax = plt.subplots(figsize=(10, 6))
                        metrics_plot = metrics_df.melt(
                            id_vars=['–°–∏—Å—Ç–µ–º–∞'],
                            value_vars=['BLEU','chrF','TER'],
                            var_name='–ú–µ—Ç—Ä–∏–∫–∞',
                            value_name='–ó–Ω–∞—á–µ–Ω–∏–µ'
                        )
                        sns.barplot(data=metrics_plot, x='–°–∏—Å—Ç–µ–º–∞', y='–ó–Ω–∞—á–µ–Ω–∏–µ', hue='–ú–µ—Ç—Ä–∏–∫–∞', ax=ax)
                        ax.set_title('–ö–æ—Ä–ø—É—Å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —Å–∏—Å—Ç–µ–º–∞–º')
                        ax.set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ')
                        ax.set_xlabel('–°–∏—Å—Ç–µ–º–∞')
                        ax.legend(title='–ú–µ—Ç—Ä–∏–∫–∞')
                        plt.tight_layout()
                        st.session_state.fig = fig
                    else:
                        st.warning("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫.")
                        st.session_state.fig = None
                else:
                    st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫.")
                    st.session_state.fig = None

            st.session_state.metrics_df = metrics_df
            st.session_state.fig = fig
            
            update_progress(10, 10, "–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            st.success("‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
            
        except Exception as e:
            st.session_state.error_occurred = True
            st.session_state.error_message = str(e)
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–æ–≤: {str(e)}")
            st.text(traceback.format_exc())
        
        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if not st.session_state.error_occurred:
            display_results(
                st.session_state.df, 
                st.session_state.processed_systems,
                source_file=uploaded_source,
                ref_file=uploaded_ref,
                model1_file=model1_zip,
                model2_file=model2_zip,
                yandex_file=yandex_file,
                google_file=google_file,
                deepl_file=deepl_file,
                extra_files=extra_files,
                extra_names=extra_names,
                model1_name=model1_name,
                model2_name=model2_name
            )
            
            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            if st.session_state.metrics_df is not None:
                st.subheader("üìà –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–æ—Ä–ø—É—Å—É")
                st.dataframe(st.session_state.metrics_df, use_container_width=True)
                
                st.subheader("üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫")
                st.pyplot(st.session_state.fig)
            
            st.subheader("‚¨áÔ∏è –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            
            # TMX –¥–ª—è –∫–∞–∂–¥–æ–π —Å–∏—Å—Ç–µ–º—ã
            tmx_col1, tmx_col2 = st.columns(2)
            for idx, system in enumerate(st.session_state.processed_systems):
                if system in st.session_state.translations:
                    tmx_bytes = export_tmx(
                        st.session_state.src_sentences, 
                        st.session_state.translations[system], 
                        system
                    )
                    # –ß–µ—Ä–µ–¥—É–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∫–Ω–æ–ø–æ–∫
                    with tmx_col1 if idx % 2 == 0 else tmx_col2:
                        st.download_button(
                            f"TMX –¥–ª—è {system}", 
                            tmx_bytes, 
                            file_name=f"{system}.tmx", 
                            mime="application/octet-stream",
                            key=f"tmx_download_{system}_{idx}"  # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
                        )
            
            # ZIP –∞—Ä—Ö–∏–≤
            if uploaded_source:
                dt = datetime.datetime.now().strftime("%Y-%m-%dT%H-%M-%S")
                src_folder = os.path.splitext(os.path.basename(uploaded_source.name))[0]
                zip_name = f"{dt}_{src_folder}.zip"
                
                zip_buffer = create_zip_archive(
                    st.session_state.src_sentences,
                    st.session_state.ref_sentences,
                    st.session_state.processed_systems,
                    [],
                    st.session_state.df,
                    st.session_state.metrics_df,
                    st.session_state.fig,
                    f"{dt}_{src_folder}"
                )
                
                st.download_button(
                    "–°–∫–∞—á–∞—Ç—å –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (ZIP)", 
                    zip_buffer, 
                    file_name=zip_name, 
                    mime="application/zip",
                    key=f"zip_download_{dt}"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º timestamp –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
                )

# –û—Å–Ω–æ–≤–Ω–æ–π –±–ª–æ–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
if __name__ == "__main__":
    main()

# --- –ö–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞ ---